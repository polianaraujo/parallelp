# Relat√≥rio 16 - Comunica√ß√£o Coletiva com MPI
Aluna: Poliana Ellen de Ara√∫jo

## 1. Introdu√ß√£o

Este reposit√≥rio cont√©m um programa MPI em C para calcular o produto y=A‚ãÖx, onde A √© uma matriz $M√óN$ e $x$ √© um vetor de tamanho $N$. O objetivo √© demonstrar e analisar o desempenho da paraleliza√ß√£o usando MPI em diferentes cen√°rios de tamanho de matriz e n√∫mero de processos.

A multiplica√ß√£o de matrizes por vetores √© uma opera√ß√£o fundamental em diversas √°reas da computa√ß√£o cient√≠fica e engenharia. Para grandes dimens√µes, a execu√ß√£o sequencial pode ser demorada. Este projeto explora como a paraleliza√ß√£o usando o padr√£o de Message Passing Interface (MPI) pode acelerar essa opera√ß√£o, distribuindo a carga de trabalho entre m√∫ltiplos processos.

### Conte√∫do

- ``matrix_vector_mpi.c``: O c√≥digo-fonte em C da implementa√ß√£o MPI.
- ``run_experiments.sh``: Um script Bash para compilar e executar o programa com v√°rias configura√ß√µes de matriz e n√∫mero de processos.
- ``resultados.csv``: Arquivo CSV que armazena os tempos de execu√ß√£o coletados pelos experimentos.
- ``README.md``: Este arquivo.

## 2. Metodologia

O programa implementa a multiplica√ß√£o y=A‚ãÖx da seguinte forma:

1. Divis√£o da Matriz A: A matriz A √© dividida por linhas entre os processos usando MPI_Scatter. Cada processo recebe um subconjunto de linhas da matriz.
2. Distribui√ß√£o do Vetor x: O vetor x completo √© broadcast para todos os processos usando MPI_Bcast, garantindo que cada processo tenha acesso a todos os elementos de x.
3. C√°lculo Local: Cada processo calcula os elementos de y correspondentes √†s suas linhas da matriz A e ao vetor x.
4. Agrega√ß√£o dos Resultados: Os resultados parciais (local_y) de cada processo s√£o coletados de volta ao processo raiz usando MPI_Gather, que os combina para formar o vetor y final.
5. Medi√ß√£o de Tempo: O tempo de execu√ß√£o √© medido do in√≠cio da distribui√ß√£o dos dados at√© o final da coleta dos resultados.

O script run_experiments.sh automatiza a execu√ß√£o do programa para os seguintes par√¢metros:
- Tamanhos da Matriz M: 1000, 10000, 50000
- Tamanhos do Vetor N: 100, 1000
- N√∫mero de Processos (NP): 2, 4, 8
√â feita uma verifica√ß√£o para garantir que M seja divis√≠vel por NP para uma distribui√ß√£o equitativa.

### Compila√ß√£o

```
sbatch job_submit.sh
```

## 3. Resultados

![Compara√ß√£o das afinidades de todas as threads](https://github.com/polianaraujo/parallelp/blob/main/tarefa16/tempo_vs_processos_tamanhos_matrizes.png)

## 4. Conclus√µes

- üìà Tend√™ncias observadas
    - ‚úÖ Efici√™ncia do paralelismo:
        - A paraleliza√ß√£o melhora o desempenho at√© certo ponto.
        - O n√∫mero √≥timo de processos varia com o tamanho do problema (M e N).

- ‚ùå Overhead de comunica√ß√£o:
    - Quando M ou N s√£o pequenos, muitos processos introduzem mais overhead do que benef√≠cio.
    - Quando M √© grande o suficiente, o paralelismo compensa o overhead ‚Äî mas at√© um certo ponto.


- Problemas pequenos (ex: M=1000, N=100): poucos processos j√° s√£o suficientes; muitos processos n√£o trazem ganho significativo.

- Problemas m√©dios (ex: M=10000, N=1000): 4 a 8 processos s√£o eficazes, mas deve-se monitorar a sobrecarga.

- Problemas grandes (ex: M=50000, N=1000): 4 processos parecem ser o ponto de equil√≠brio; 8 processos aumentam a comunica√ß√£o e n√£o compensam em desempenho.