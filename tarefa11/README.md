# Relat√≥rio 11 - Impacto das cl√°usulas schedule e collapse
Aluna: Poliana Ellen de Ara√∫jo

## 1. Introdu√ß√£o

Este relat√≥rio apresenta uma **simula√ß√£o do movimento de um fluido ao longo do tempo usando a equa√ß√£o de Navier-Stokes**, considerando apenas os efeitos da viscosidade, desconsiderando a press√£o e quaisquer for√ßas externas. Utilizando **diferen√ßas finitas para discretizar o espa√ßo e simule a evolu√ß√£o da velocidade do fluido no tempo**. Foi pedido para inicializar o fluido parado ou com velocidade constante e verificar se o campo permanece est√°vel, e em seguida inserido uma pequena perturba√ß√£o. Ap√≥s validar, o c√≥digo foi paralelizado com OpenMP explorando o impacto das cl√°usulas schedule e collapse no desempenho da execu√ß√£o paralela.
Portante, foram realizados dois conjuntos distintos de experimentos:
- Com Perturba√ß√£o Inicial (vers√£o sequencial): utilizada para visualiza√ß√£o do comportamento da simula√ß√£o ao longo do tempo.
- Sem Perturba√ß√£o Inicial (vers√£o paralela): utilizada para testes de desempenho isolados, eliminando efeitos de valores iniciais n√£o-nulos.


A equa√ß√£o da viscosidade (sem press√£o e for√ßas externas) em duas dimens√µes √© dada por:

$$ \frac{ùúïu}{ùúït}=vùõª^{2}u $$

Onde:
- $u$: Campo de velocidade do fluido
- $v$: Coeficiente de viscosidade
- $ùõª^{2}$: Operador Laplaciano

![navier_stokes_diffusion](navier_stokes_diffusion.gif)


## 2. Metodologia

### 2.1. C√≥digo Sequencial - Equa√ß√£o considerando apenas a viscosidade
- Defini√ß√£o de Constantes:
    - `NX` e `NY`: Dimens√µes da grade, nesse caso `100x100`.
    - `NSTEPS`: N√∫mero de passos de tempo, nesse caso `1000`.
    - `DT`: Passo de tempo, nesse caso `0,1`.
    - `VISCOSITY`: Coeficiente de viscosidade, nesse caso `0,1`.
- Fun√ß√£o ``initialize_field``:
    - Inicializa o campo de velocidade com zeros.
    - Introduz uma perturba√ß√£o no centro da grade.
- Fun√ß√£o ``update_field``:
    - Calcula a pr√≥xima itera√ß√£o do campo (como ele se comporta com o tempo) usando a f√≥rmula discreta da equa√ß√£o de Navier-Stokes.
- Fun√ß√£o ``simulate``:
    - Realiza a simula√ß√£o iterativa, em `NSTEPS`.
    - Em cada itera√ß√£o, a fun√ß√£o update_field √© chamada para atualizar o campo (simula√ß√£o de como a velocidade do fluido, em cada ponto da malha, muda ao longo do tempo, considerando apenas os efeitos da difus√£o viscosa).
- Fun√ß√£o ``print_field``:
    - Imprime o campo em formato simplificado para monitoramento.
- Fun√ß√£o ``main``:
    - Inicializa o campo, executa a simula√ß√£o calculando o tempo atrav√©s da fun√ß√£o `gettimeofday()`.

### 2.2. C√≥digo Paralelo - Com OpenMP

Paralelizar a simula√ß√£o utilizando OpenMP para explorar m√∫ltiplos threads e melhorar o desempenho. O paralelismo foi aplicado nas seguintes regi√µes:
- C√°lculo do laplaciano interno da matriz.
- C√≥pia da matriz next para current a cada passo temporal.
O c√≥digo foi executado **com e sem perturba√ß√£o**(linha 21 comentada).

Tr√™s tipos de escalonamento foram testados:
- ``static``
- `dynamic`
- `guided`

Com tr√™s tamanhos de chunk: 1, 4 e 8. O n√∫mero de threads variou entre 1, 2, 4, 8 e 16. Para garantir consist√™ncia nas medi√ß√µes, a inicializa√ß√£o foi feita sem perturba√ß√£o (campo inicializado com zeros). O tempo de execu√ß√£o foi medido com `omp_get_wtime()`.

- Inclus√£o da Biblioteca OpenMP:
    - `#include <omp.h>`
- Ajuste da Fun√ß√£o `update_field`:
    - A diretiva `#pragma omp parallel for collapse(2)` `schedule(static)` foi adicionada.
        - `parallel for`: Indica que o loop ser√° paralelizado.
        - `collapse(2)`: Unifica os dois loops aninhados (i e j) em um √∫nico loop virtual, permitindo um balanceamento mais eficiente da carga de trabalho.
        - `schedule(static)`: Divide o loop em blocos iguais e atribui cada bloco a um thread (`dynamic` e `guided` s√£o outras op√ß√µes, que ajustam a divis√£o dinamicamente ou de forma guiada).

Para avaliar o impacto da paraleliza√ß√£o no desempenho da simula√ß√£o, foi implementado no c√≥digo a fun√ß√£o `run_simulation`, que permite executar a simula√ß√£o com diferentes configura√ß√µes:
- Varia√ß√£o do n√∫mero de threads (1, 2, 4, 8, 16), com o intuito de investigar o efeito do grau de paralelismo.
- Diferentes tipos de escalonamento (`static`, `dynamic`, `guided`), servindo para determinar como as intera√ß√µes do loop paralelo s√£o distruibu√≠das entre as threads
- Diferentes tamanhos de chunk (1, 4, 8), definindo a granularidade das tarefas atribu√≠das a cada thread.

### 2.3. Execu√ß√£o

A execu√ß√£o foi realizada com os seguintes comandos, para o sequencial e o paralelo, respectivamente:

```
gcc -o sequential sequential.c -lm
./sequential
gcc -o parallel parallel.c -fopenmp -lm
./parallel
```

A vers√£o paralela foi executada duas vezes, a primeira com a aplica√ß√£o de perturba√ß√£o, atrav√©s da linha 21 abaixo, e o resultado foi salvo em um arquivo csv chamado results_w_perturbacao.csv:
```
field[NX / 2][NY / 2] = 1.0f;
```

## 3. Resultados

Enquanto os dados da execu√ß√£o sem perturba√ß√£o (transformando a linha acima comentada), foram salvos em um arquivo chamado results_sem_perturbacao.csv.

Em ambos, os dados foram coletados cont√©m:
- N√∫mero de threads utilizadas
- Tipo de escalonamento
- Tamanho do chunk
- Tempo total de execu√ß√£o
- Valor final no centro da matriz
- Valor m√©dio de toda a matriz

|Sem perturba√ß√£o|Com perturba√ß√£o|
|-----|-----|
|![combined_analysis_sem_perturb](https://github.com/polianaraujo/parallelp/blob/main/tarefa11/plots_sem_perturb/combined_analysis_sem_perturb.png)|![Combina√ß√£o](https://github.com/polianaraujo/parallelp/blob/main/tarefa11/plots_w_perturb/combined_analysis_w_perturb.png)|
|![Combina√ß√£o](https://github.com/polianaraujo/parallelp/blob/main/tarefa11/plots_sem_perturb/time_per_chunk_sem_perturb.png)|![Combina√ß√£o](https://github.com/polianaraujo/parallelp/blob/main/tarefa11/plots_w_perturb/time_per_chunk_w_perturb.png)|

### 3.1 Sem perturba√ß√£o

üîπ `combined_analysis_sem_perturb`: Threads √ó Schedule √ó Tempo
Este gr√°fico compara os tempos de execu√ß√£o em fun√ß√£o da quantidade de threads e do tipo de escalonamento utilizado (static, dynamic, guided). De modo geral, observam-se as seguintes tend√™ncias:

Melhor desempenho (menor tempo) √© obtido com o escalonamento guided, especialmente quando o n√∫mero de threads aumenta at√© 8. Para 2, 4 e 8 threads, guided apresenta tempos significativamente menores que os demais tipos.

O escalonamento dynamic apresenta maior variabilidade, e em alguns casos (como com 4 e 8 threads), resulta em tempos superiores aos outros m√©todos.

Com 16 threads, todos os tipos de escalonamento apresentam um aumento abrupto no tempo de execu√ß√£o, provavelmente devido √† sobrecarga de paralelismo e/ou limita√ß√£o de recursos computacionais. Isso sugere inefici√™ncia na utiliza√ß√£o de muitos threads para esse problema espec√≠fico.

O tempo de execu√ß√£o diminui gradualmente com o aumento do n√∫mero de threads at√© 8, e volta a crescer com 16 threads, indicando um ponto √≥timo pr√≥ximo de 4 a 8 threads, dependendo do escalonamento.

üîπ `time_per_chunk_sem_perturb`: Chunk Size √ó Tempo
Neste gr√°fico, analisamos o tempo de execu√ß√£o em rela√ß√£o ao tamanho do chunk (1, 4, 8) para cada combina√ß√£o de threads e tipo de escalonamento.

Para a maioria dos casos, um tamanho de chunk menor (1) resulta em maior tempo de execu√ß√£o, especialmente com escalonamento dynamic. Isso se deve ao overhead de agendamento muito frequente.

Com o aumento do tamanho do chunk (4 e 8), o tempo tende a reduzir at√© certo ponto, pois o agendamento ocorre com menos frequ√™ncia.

No escalonamento guided, os tempos s√£o consistentemente baixos independentemente do chunk size, o que demonstra a efici√™ncia adaptativa deste tipo de escalonamento.

No escalonamento static, a diferen√ßa entre tamanhos de chunk √© pequena, o que √© esperado, visto que a divis√£o de trabalho √© definida previamente e n√£o muda durante a execu√ß√£o.

### 3.2 Com perturba√ß√£o

üîπ `combined_analysis_com_perturb`: Threads x Schedule x Tempo
Este gr√°fico permite comparar o tempo de execu√ß√£o para diferentes combina√ß√µes de n√∫mero de threads e estrat√©gias de escalonamento (static, dynamic e guided) com chunk sizes variados, em um cen√°rio com perturba√ß√µes.

Observa-se que, para at√© 8 threads, o tempo de execu√ß√£o tende a reduzir com o aumento do n√∫mero de threads, especialmente nos agendamentos guided e static.

Para 16 threads, todos os escalonamentos apresentam um tempo de execu√ß√£o mais alto e relativamente est√°vel, indicando poss√≠vel overhead de paralelismo em excesso ou conten√ß√£o de recursos, comum em sistemas com muitos threads competindo simultaneamente.

O agendamento guided mostrou o melhor desempenho geral para 4 e 8 threads, mantendo tempos de execu√ß√£o baixos e consistentes.

O escalonamento dynamic teve desempenho inferior para 1 e 2 threads, mas se aproximou do desempenho dos outros √† medida que as threads aumentaram.

Em todos os agendamentos, o chunk size afeta pouco o desempenho para guided, mas causa maior varia√ß√£o nos modos static e dynamic.

üîπ `time_per_chunk_com_perturb`: Tempo por Chunk Size
Este gr√°fico mostra o tempo m√©dio de execu√ß√£o por chunk size (1, 4 e 8), agrupando todos os valores de threads e escalonamentos.

Observa-se uma tend√™ncia de queda no tempo m√©dio de execu√ß√£o com o aumento do chunk size. Isso √© esperado, pois maiores chunk sizes reduzem a sobrecarga de agendamento.

Chunk size 1 apresenta o maior tempo m√©dio, pois exige agendamento mais frequente e granular, sendo mais sens√≠vel √†s perturba√ß√µes.

Chunk sizes 4 e 8 s√£o mais eficientes e robustos √† perturba√ß√£o, especialmente no modo static.


## 4. Conclus√µes

- A pol√≠tica de escalonamento `guided` apresentou os melhores resultados de tempo para escalabilidade forte, especialmente com baixo n√∫mero de threads (1 a 4).
- A pol√≠tica `dynamic` n√£o √© adequada para escalabilidade fraca, pois demonstrou altos tempos de execu√ß√£o e baixa efici√™ncia, especialmente com maiores quantidades de threads.
- A melhor performance global foi alcan√ßada com at√© 4 threads, sendo que, a partir disso, o overhead de gerenciamento de threads parece superar o ganhos de paralelismo.
- A cl√°usula `collapse` teve impacto vari√°vel, mas em geral, valores mais baixos (`collapse=1`) apresentaram melhor desempenho em cen√°rios com muitas threads.
- A an√°lise sugere que o uso de paralelismo deve ser balanceado com o tamanho da carga de trabalho, evitando o uso excessivo de threads em problemas pequenos.
- O desempenho √≥timo depende da combina√ß√£o entre pol√≠tica de escalonamento, n√∫mero de threads e estrutura de la√ßos (`collapse`). Para cargas maiores, a pol√≠tica `static` ou `guided` com ajustes adequados pode ser a mais indicada.
