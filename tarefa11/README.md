# Relat√≥rio 11 - Impacto das cl√°usulas schedule e collapse
Aluna: Poliana Ellen de Ara√∫jo

## 1. Introdu√ß√£o

Este relat√≥rio apresenta uma **simula√ß√£o do movimento de um fluido ao longo do tempo usando a equa√ß√£o de Navier-Stokes**, considerando apenas os efeitos da viscosidade, desconsiderando a press√£o e quaisquer for√ßas externas. Utilizando **diferen√ßas finitas para discretizar o espa√ßo e simule a evolu√ß√£o da velocidade do fluido no tempo**. Foi pedido para inicializar o fluido parado ou com velocidade constante e verificar se o campo permanece est√°vel, e em seguida inserido uma pequena perturba√ß√£o. Ap√≥s validar, o c√≥digo foi paralelizado com OpenMP explorando o impacto das cl√°usulas schedule e collapse no desempenho da execu√ß√£o paralela.
Portante, foram realizados dois conjuntos distintos de experimentos:
- Com Perturba√ß√£o Inicial (vers√£o sequencial): utilizada para visualiza√ß√£o do comportamento da simula√ß√£o ao longo do tempo.
- Sem Perturba√ß√£o Inicial (vers√£o paralela): utilizada para testes de desempenho isolados, eliminando efeitos de valores iniciais n√£o-nulos.


A equa√ß√£o da viscosidade (sem press√£o e for√ßas externas) em duas dimens√µes √© dada por:
$$ \frac{ùúïu}{ùúït}=vùõª^{2}u $$
Onde:
- $u$: Campo de velocidade do fluido
- $v$: Coeficiente de viscosidade
- $ùõª^{2}$: Operador Laplaciano

![navier_stokes_diffusion](navier_stokes_diffusion.gif)


## 2. Metodologia

### 2.1. C√≥digo Sequencial - Equa√ß√£o considerando apenas a viscosidade
- Defini√ß√£o de Constantes:
    - `NX` e `NY`: Dimens√µes da grade, nesse caso `100x100`.
    - `NSTEPS`: N√∫mero de passos de tempo, nesse caso `1000`.
    - `DT`: Passo de tempo, nesse caso `0,1`.
    - `VISCOSITY`: Coeficiente de viscosidade, nesse caso `0,1`.
- Fun√ß√£o ``initialize_field``:
    - Inicializa o campo de velocidade com zeros.
    - Introduz uma perturba√ß√£o no centro da grade.
- Fun√ß√£o ``update_field``:
    - Calcula a pr√≥xima itera√ß√£o do campo (como ele se comporta com o tempo) usando a f√≥rmula discreta da equa√ß√£o de Navier-Stokes.
- Fun√ß√£o ``simulate``:
    - Realiza a simula√ß√£o iterativa, em `NSTEPS`.
    - Em cada itera√ß√£o, a fun√ß√£o update_field √© chamada para atualizar o campo (simula√ß√£o de como a velocidade do fluido, em cada ponto da malha, muda ao longo do tempo, considerando apenas os efeitos da difus√£o viscosa).
- Fun√ß√£o ``print_field``:
    - Imprime o campo em formato simplificado para monitoramento.
- Fun√ß√£o ``main``:
    - Inicializa o campo, executa a simula√ß√£o calculando o tempo atrav√©s da fun√ß√£o `gettimeofday()` e exibe os resultados na Figura 1.

Portanto

### 2.2. C√≥digo Paralelo - Com OpenMP

Paralelizar a simula√ß√£o utilizando OpenMP para explorar m√∫ltiplos threads e melhorar o desempenho. O paralelismo foi aplicado nas seguintes regi√µes:
- C√°lculo do laplaciano interno da matriz.
- C√≥pia da matriz next para current a cada passo temporal.

Tr√™s tipos de escalonamento foram testados:
- ``static``
- `dynamic`
- `guided`

Com tr√™s tamanhos de chunk: 1, 4 e 8. O n√∫mero de threads variou entre 1, 2, 4, 8 e 16. Para garantir consist√™ncia nas medi√ß√µes, a inicializa√ß√£o foi feita sem perturba√ß√£o (campo inicializado com zeros). O tempo de execu√ß√£o foi medido com `omp_get_wtime()`.

- Inclus√£o da Biblioteca OpenMP:
    - `#include <omp.h>`
- Ajuste da Fun√ß√£o `update_field`:
    - A diretiva `#pragma omp parallel for collapse(2)` `schedule(static)` foi adicionada.
        - `parallel for`: Indica que o loop ser√° paralelizado.
        - `collapse(2)`: Unifica os dois loops aninhados (i e j) em um √∫nico loop virtual, permitindo um balanceamento mais eficiente da carga de trabalho.
        - `schedule(static)`: Divide o loop em blocos iguais e atribui cada bloco a um thread (`dynamic` e `guided` s√£o outras op√ß√µes, que ajustam a divis√£o dinamicamente ou de forma guiada).

Para avaliar o impacto da paraleliza√ß√£o no desempenho da simula√ß√£o, foi implementado no c√≥digo a fun√ß√£o `run_simulation`, que permite executar a simula√ß√£o com diferentes configura√ß√µes:
- Varia√ß√£o do n√∫mero de threads (1, 2, 4, 8, 16), com o intuito de investigar o efeito do grau de paralelismo.
- Diferentes tipos de escalonamento (`static`, `dynamic`, `guided`), servindo para determinar como as intera√ß√µes do loop paralelo s√£o distruibu√≠das entre as threads
- Diferentes tamanhos de chunk (1, 4, 8), definindo a granularidade das tarefas atribu√≠das a cada thread.

### 2.3. Execu√ß√£o

A execu√ß√£o foi realizada com os seguintes comandos, para o sequencial e o paralelo, respectivamente:

```
gcc -o sequential sequential.c -lm
./sequential
gcc -o parallel parallel.c -fopenmp -lm
./parallel
```

A vers√£o paralela foi executada duas vezes, a primeira com a aplica√ß√£o de perturba√ß√£o, atrav√©s da linha 21 abaixo, e o resultado foi salvo em um arquivo csv chamado results_w_perturbacao.csv:
```
field[NX / 2][NY / 2] = 1.0f;
```

Enquanto os dados da execu√ß√£o sem perturba√ß√£o (transformando a linha acima comentada), foram salvos em um arquivo chamado results_sem_perturbacao.csv.

Em ambos, os dados foram coletados cont√©m:
- N√∫mero de threads utilizadas
- Tipo de escalonamento
- Tamanho do chunk
- Tempo total de execu√ß√£o
- Valor final no centro da matriz
- Valor m√©dio de toda a matriz

## 3. Resultados

|Escalabilidade forte|Escalabilidade fraca|
|-----|-----|
|![Escalabilidade forte](https://github.com/polianaraujo/parallelp/blob/main/tarefa12/graficos/escalabilidade_forte.png)|![Escalabilidade fraca](https://github.com/polianaraujo/parallelp/blob/main/tarefa12/graficos/escalabilidade_fraca.png)|

### 3.1. Escalabilidade Forte
Com o tamanho do problema fixo (`N = 128`), podendo observar:

- Com 1 thread, o tempo de execu√ß√£o varios entre ~1.2 e 1.7 segundos, dependendo da pol√≠tica de escalonamento e da cl√°usula `collaspse`. O melhor tempo foi com `guided` e `caollpase=3`.
- A redu√ß√£o no tempo de execu√ß√£o foi not√°vel com o aumento de threads at√© certo ponto. Com 2 e 4 threads, o tempo caiu consideralvemente, alcan√ßando valores em torno de 0.27s e 0.68s.
- Com 8 threads, houve bons resultados com `guided` e `dynamic`, especialmente para `collapse=1`, com tempos pr√≥ximos a 0.28s.
- A partir de 16 threads, observou-se uma diminui√ß√£o nos ganhos de desempenho. Em alguns casos, como `collapse=3`, o tempo voltou a aumentar.
- Com `128` threads, o desempenho piorou, com tempos que se aproximaram novamente de 1.4-1.7s, sugerindo overhead de paraleliza√ß√£o.

### 3.2. Escalabilidade Fraca

O tamanho do problema cresce proporcionalmente ao n√∫mero de threads.
- Com 1 thread e `N=50`, o tempo foi cerca de 0.27s para todas as pol√≠ticas e valores de `collapse`, exceto `dynamic`, que apresentou valores pr√≥ximos a 0.79s.
- Com 2 threads (`N=100`), os tempos aumentaram para ~`1.56s` para `static` e `guided`, e ultrapassaram 5s com `dynamic`, mostrando um desempenho ruim com esse pol√≠tica.
- Com 4 threads (`N=200`), `static` e `guided` mantiveram tempos ao redor de 13s, enquanto `dynamic` foi significativamente maior, chegando a 42s.
- Com 8 threads (`N=400`), os tempos com `static` subiram para mais de `100s`, enquanto `dynamic` atingiu pucos acima de 340s, evidenciando grande inefici√™ncia.

## 4. Conclus√µes

- A pol√≠tica de escalonamento `guided` apresentou os melhores resultados de tempo para escalabilidade forte, especialmente com baixo n√∫mero de threads (1 a 4).
- A pol√≠tica `dynamic` n√£o √© adequada para escalabilidade fraca, pois demonstrou altos tempos de execu√ß√£o e baixa efici√™ncia, especialmente com maiores quantidades de threads.
- A melhor performance global foi alcan√ßada com at√© 4 threads, sendo que, a partir disso, o overhead de gerenciamento de threads parece superar o ganhos de paralelismo.
- A cl√°usula `collapse` teve impacto vari√°vel, mas em geral, valores mais baixos (`collapse=1`) apresentaram melhor desempenho em cen√°rios com muitas threads.
- A an√°lise sugere que o uso de paralelismo deve ser balanceado com o tamanho da carga de trabalho, evitando o uso excessivo de threads em problemas pequenos.
- O desempenho √≥timo depende da combina√ß√£o entre pol√≠tica de escalonamento, n√∫mero de threads e estrutura de la√ßos (`collapse`). Para cargas maiores, a pol√≠tica `static` ou `guided` com ajustes adequados pode ser a mais indicada.
