# Relat√≥rio 15 - Difus√£o de calor usando MPI
Aluna: Poliana Ellen de Ara√∫jo

## 1. Introdu√ß√£o


## 2. Metodologia

### 2.1. Vers√£o MPI_Send / MPI_Recv `heat_diffusion_denrecv.c`

- Comunica√ß√£o bloqueante: processo envia e espera receber dados antes de continuar a computa√ß√£o.
- Troca de bordas primeiro, s√≥ depois atualiza o dom√≠nio.

üîÅ Comunica√ß√£o
```
MPI_Send(...);
MPI_Recv(...);
```

üìå Ordem das chamadas
1. Envia para vizinhos.
2. Recebe dos vizinhos.
3. Ap√≥s recebimento, atualiza toda a regi√£o.

‚ö†Ô∏è Ponto de aten√ß√£o
- Sincroniza√ß√£o estrita: cada processo s√≥ avan√ßa quando os dados chegam.
- N√£o h√° sobreposi√ß√£o entre comunica√ß√£o e computa√ß√£o.

‚è±Ô∏è Desempenho
- Pode ser mais lento, especialmente em sistemas com lat√™ncia alta.

### 2.2. Vers√£o MPI_Isend / MPI_Irecv + MPI_Wait `heat_diffusion_isend_wait.c`

- Comunica√ß√£o n√£o bloqueante (MPI_Irecv, MPI_Isend) permite iniciar comunica√ß√µes e s√≥ esperar no final.
- Mas ainda espera por todas antes de computar.

üîÅ Comunica√ß√£o
```
MPI_Irecv(...);  // iniciar recep√ß√£o
MPI_Isend(...);  // iniciar envio
MPI_Waitall(...); // espera tudo terminar
```

üß† Vantagem
A comunica√ß√£o √© ass√≠ncrona, e isso abre caminho para sobreposi√ß√£o, mas ela ainda n√£o √© explorada aqui.

‚ö†Ô∏è Ponto de aten√ß√£o
- Espera por todas as mensagens antes de atualizar os valores.
- Nenhuma computa√ß√£o √© feita antes do MPI_Waitall.

‚è±Ô∏è Desempenho
Geralmente melhor que Send/Recv, mas ainda n√£o √≥timo.

### 2.3. Vers√£o MPI_Test `heat_diffusion_overlap.c`

- Comunica√ß√£o iniciada com MPI_Irecv e MPI_Isend.
- Enquanto espera os dados das bordas, j√° computa os pontos internos.
- Usa MPI_Test para checar se os dados chegaram. Se n√£o chegaram, usa MPI_Wait.

```
MPI_Irecv(...);
MPI_Isend(...);
...
// Computa pontos internos enquanto espera comunica√ß√£o
MPI_Test(...); // checa se dados chegaram
MPI_Wait(...); // se necess√°rio, espera dados
```

üß† Vantagem
- Sobreposi√ß√£o real de comunica√ß√£o e computa√ß√£o:
    - Enquanto a mensagem n√£o chega, o processo avan√ßa no c√°lculo.
    - Isso esconde a lat√™ncia da comunica√ß√£o.

‚è±Ô∏è Desempenho
- Tend√™ncia a ser a mais r√°pida entre as tr√™s.
- Ideal quando o custo de comunica√ß√£o e computa√ß√£o s√£o compar√°veis.

## 3. Resultados

|Vers√£o|Comunica√ß√£o|Computa√ß√£o sobreposta?|Tempo|Desempenho|
|------|-----------|----------------------|-----|----------|
|``MPI_Send`` / ``MPI_Recv``|Bloqueante|‚ùå N√£o|0.329468 s|üö´ Mais lento|
|``MPI_Isend`` / ``MPI_Irecv`` + ``Wait``|N√£o bloqueante|‚ùå N√£o|0.001555 s|‚ö†Ô∏è Intermedi√°rio|
|``MPI_Test`` + sobreposi√ß√£o|N√£o bloqueante|‚ùå N√£o|0.001590 s|‚úÖ Melhor desempenho|


## 4. Conclus√µes

- A vers√£o MPI_Send/Recv √© mais simples, mas bloqueia o processo enquanto espera os dados. √â adequada para testes ou casos onde comunica√ß√£o n√£o √© gargalo.
- A vers√£o MPI_Isend/Irecv + Wait melhora a escalabilidade e prepara o terreno para computa√ß√£o sobreposta, mas ainda espera todos os dados antes de computar.
- A vers√£o MPI_Test permite esconder o custo de comunica√ß√£o com computa√ß√£o √∫til, otimizando o uso dos recursos do processador.